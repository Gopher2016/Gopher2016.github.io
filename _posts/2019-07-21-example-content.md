---
layout: post
title: Text Classification of Medical Records by Clinical Domain
---

Within the the field of medicine there are a number of practices in which physicians may choose to specialize. These subspecialties of medicine include everything from general medicine, which deals with with the prevention, diagnosis, and treatment of disease, to highly specialized fields concerned with the anatomy and function of complex systems within the human body, including fields like cardiology and neurology to name a few. The vast number of subspecialties within the field of medicine often times creates a murky connotation for the role of a 'physician', a term used to describe a person qualified to practice medicine, despite the fact that two physicians working in different specialties of medicine have drastically different talents and knowledge within their respective fields.

The objective of this analysis was to collect text data from https://MTSamples.com which provides access to a large collection of transcribed medical reports across various medical specialties. All names and dates have been changed (or removed) to maintain confidentiality of hospitals, physicians, and patients. Medical records were specifically collected from five clinical domains, gastroenterology, neurology, orthopedic, radiology, and urology, each serving as labeled text data to be used for multinomial classification of medical records by specialty domain. This process would allow me to train a model using supervised learning to classify new medical records into one of the five predetermined medical specialties.

From each clinical domain medical notes were scraped using BeautifulSoup and converted to a Pandas DataFrame within Python. A total of 1,024 medical notes across all specialties were collected and the classes were reasonably balanced with 220 gastroenterology documents, 214 neurology documents, 318 orthopedic documents, 130 radiology documents, and 142 urology documents. Custom stop words that are common in medical documents were added to the default english stop words and used to vectorize text from the medical notes with scikit-learn's CountVectorizer and TF-IDF (term frequency-inverse document frequency) Vectorizer. For each classifier I decided to use the TF-IDF Vectorizer approach over the CountVectorizer approach because this approach would allow me to increase proportionality for the number of times a word appeared in each medical document, ultimately helping to adjust for the fact that some words appear more frequently than others and thus providing a term-weighting approach to vectorizing text.

Four separate models were tested on the full corpus of medical documents using a stratified k-fold cross-validation with five folds to evaluate average metric scores for each model, including weighted precision, recall, and F1 scores. Among the models that were tested included linear SVC, logistic regression, Naive Bayes, and random forest classifiers. The linear SVC classifier was found to have the highest mean weighted precision, recall, and F1 scores from the cross-validation of medical records text data. Weighted F1 scores were deemed to be the best measure of model performance since the metric considers both precision and recall to compute score. Weighted F1 scores for each fold in the cross-validation of medical records are shown below for each model represented by box plots to display the range of F1 scores across k-folds.

![Distribution](https://github.com/Gopher2016/Gopher2016.github.io/blob/master/images/Weighted%20F1%20Scores%20.png?raw=true)

The medical records dataframe was then split into train and test sets using a test size of 0.33, or 33 percent of all medical documents. Each model was fit to the training set and class predictions were made on the test set. As before, the linear SVC model performed the best across all metrics with a weighted average F1 score of 0.93, a weighted average precision score of 0.93, and a weighted average recall score of 0.93. A confusion matrix for the linear SVC model is shown below in which the vast majority of the predictions end up on the diagonal. In other words, the vast majority of predicted medical specialties from the medical notes text were matched correctly with the actual medical specialty from which the medical documents were collected.

![Distribution](https://github.com/Gopher2016/Gopher2016.github.io/blob/master/images/Linear%20SVC%20Confusion%20Matrix.png?raw=true)

In addition to evaluating classification metric scores, I chose to investigate the top unigrams and bigrams associated with each clinical domain following model fitting of the linear SVC model. Below is the output showing the top two unigrams and bigrams within each medical specialty, identified by using a chi-squared test to find terms that were most correlated with each class. Many of the terms are words that one could naively predict would be associated with each specialty, confirming that our corpus of medical documents are indeed separable from the text data within each medical record.  

![Distribution](https://github.com/Gopher2016/Gopher2016.github.io/blob/master/images/Medical%20Document%20Unigrams%20&%20Bigrams.png?raw=true)

The results from this analysis may ultimately be leveraged and expanded upon to guide further insight into the differences that exist between various disciplines of medicine. Unstructured data in the form of text exists everywhere and can be an extremely rich source of information. Text classification is one such application that has many potential use cases for providing an efficient way to enhance insights and decision-making processes. Computers are exceptional at detecting patterns and making predictions from tabular spreadsheets in the form of numerical data, however, language is the form in which humans communicate with one another and is omnipresent throughout the world. Understanding natural language processing methods and techniques for extracting insights from human language in the form of text data has far-reaching applications ranging from sentiment analysis, topic modeling, spam and fraud detection, and much more, with the prospect of dramatically improving upon the way in which we make everyday decisions in business and life.
